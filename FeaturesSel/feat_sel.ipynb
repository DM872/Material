{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DM872/Material/blob/main/FeaturesSel/feat_sel.ipynb\">,\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"\\>,\n",
    "    </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "\n",
    "## Solution Approach\n",
    "\n",
    "\n",
    "\n",
    "### Sets and Indices\n",
    "\n",
    "$i \\in I=\\{1,2,\\dots,n\\}$: Set of observations.\n",
    "\n",
    "$j \\in J=\\{0,1,2,\\dots,p\\}$: Set of features, where the first ID corresponds to the intercept.\n",
    "\n",
    "$\\ell \\in L = J \\setminus \\{0\\}$: Set of features, where the intercept is excluded.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$s \\in \\mathbb{N}$: Number of features to include in the model, ignoring the intercept.\n",
    "\n",
    "\n",
    "### Decision Variables\n",
    "\n",
    "$\\beta_j \\in \\mathbb{R}$: Weight of feature $j$, representing the change in the response variable per unit-change of feature $j$.\n",
    "\n",
    "$z_\\ell \\in \\{0,1\\}$: 1 if weight of feature $\\ell$ is exactly equal to zero, and 0 otherwise. Auxiliary variable used to manage the budget constraint.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "- **Training error**: Minimize the Sum of Abosulte Residuals (aka L1) more robust with respect to outliers:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Min} \\quad Z = \\sum_{i \\in I}\\left|y_i-\\sum_{j \\in J}\\beta_{j}x_{ij}\\right|\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- **Mutual exclusivity**: For each feature $\\ell$, if $z_\\ell=1$, then $\\left|\\beta_\\ell\\right|=0$\n",
    "\n",
    "\\begin{equation}\n",
    "\\left|\\beta_\\ell\\right|\\leq M (1-z_\\ell)\n",
    "\\end{equation}\n",
    "\n",
    "Here the big-M is the upper bound of the $\\beta$ variables, which are actually unlimited. We could introduce an artificial upper bound and we could add the lasso term in the objective function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{Min} \\quad Z = \\sum_{i \\in I}\\left|y_i-\\sum_{j \\in J}\\beta_{j}x_{ij}\\right| + \\lambda \\sum_{\\ell=1}^p\\left|\\beta\\right|\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Alternatively we can define as a Special Ordered Set of type one (meaning that at most one variables is different from zero (variables can be both integer of continuous):\n",
    "\n",
    "\\begin{equation}\n",
    "(\\beta_\\ell, z_\\ell): \\text{SOS-1} \\quad \\forall \\ell \\in L\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "- **Budget constraint**: Exactly $|L| - s$ feature weights must be equal to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{\\ell \\in L}z_\\ell = |L| - s\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "This model, by means of constraint 2, implicitly considers all ${{p} \\choose s}$ feature subsets at once. However, we also need to find the value for $s$ that maximizes the performance of the regression on unseen observations. Notice that the training error decreases monotonically as more features are considered, so it is not advisable to use it as the performance metric. Instead, we should estimate the Mean Squared Error (MSE) via cross-validation. This metric is defined as $\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}$, where $y_i$ and $\\hat{y}_i$ are the observed and predicted values for the ith observation, respectively. Then, we will fine-tune $s$ using grid search, provided that the set of possible values is quite small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data and split into train (80%) and test (20%)\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2,random_state=10101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain_std = scaler.transform(Xtrain)\n",
    "Xtest_std = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "Xtrain_std.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Academic license - for non-commercial use only - expires 2021-05-06\n",
      "Using license file /Users/marco/gurobi.lic\n",
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (mac64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 431 rows, 861 columns and 6555 nonzeros\n",
      "Model fingerprint: 0x0cd87ed7\n",
      "Variable types: 848 continuous, 13 integer (13 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [3e-04, 1e+04]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [5e+00, 1e+04]\n",
      "Found heuristic solution: objective 9112.8000000\n",
      "Presolve time: 0.02s\n",
      "Presolved: 431 rows, 861 columns, 6555 nonzeros\n",
      "Variable types: 848 continuous, 13 integer (13 binary)\n",
      "\n",
      "Root relaxation: objective 1.248587e+03, 451 iterations, 0.04 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 1248.58702    0    9 9112.80000 1248.58702  86.3%     -    0s\n",
      "H    0     0                    1589.0568379 1248.58702  21.4%     -    0s\n",
      "     0     0 1248.58702    0    8 1589.05684 1248.58702  21.4%     -    0s\n",
      "     0     0 1248.58702    0    8 1589.05684 1248.58702  21.4%     -    0s\n",
      "     0     0 1248.58702    0    8 1589.05684 1248.58702  21.4%     -    0s\n",
      "     0     0 1248.58702    0    9 1589.05684 1248.58702  21.4%     -    0s\n",
      "     0     0 1248.58702    0    9 1589.05684 1248.58702  21.4%     -    0s\n",
      "     0     0 1248.58702    0    9 1589.05684 1248.58702  21.4%     -    1s\n",
      "     0     0 1248.58702    0    9 1589.05684 1248.58702  21.4%     -    1s\n",
      "H    0     0                    1539.9817901 1248.58702  18.9%     -    1s\n",
      "H    0     0                    1496.0014451 1248.58702  16.5%     -    1s\n",
      "     0     2 1248.58702    0    9 1496.00145 1248.58702  16.5%     -    1s\n",
      "*   19    12               7    1420.3735309 1248.58702  12.1%  35.5    1s\n",
      "*   28    12              10    1406.2068025 1272.87343  9.48%  40.1    1s\n",
      "*   81    16              10    1377.4736709 1276.13339  7.36%  30.6    1s\n",
      "*   84    16               9    1368.6197780 1276.13339  6.76%  30.7    1s\n",
      "*   92    16              11    1367.9471179 1276.13339  6.71%  29.9    1s\n",
      "H  110    16                    1360.5415651 1282.02361  5.77%  28.3    1s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 6\n",
      "\n",
      "Explored 189 nodes (4837 simplex iterations) in 1.65 seconds\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 10: 1360.54 1367.95 1368.62 ... 9112.8\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.360541565095e+03, best bound 1.360541565095e+03, gap 0.0000%\n",
      "\n",
      "Sum of Absolute Residuals (Training Error): 1360.54\n",
      "\n",
      "betas:\n",
      "0 21.7699\n",
      "6 3.65536\n",
      "8 -0.953577\n",
      "11 -1.74308\n",
      "12 1.64879\n",
      "13 -3.5069\n",
      "\n",
      "zetas:\n",
      "1 1\n",
      "2 1\n",
      "3 1\n",
      "4 1\n",
      "5 1\n",
      "6 -0\n",
      "7 1\n",
      "8 -0\n",
      "9 1\n",
      "10 1\n",
      "11 -0\n",
      "12 -0\n",
      "13 0\n"
     ]
    }
   ],
   "source": [
    "m = gp.Model(\"fitting\")\n",
    "\n",
    "# Sets\n",
    "p = Xtrain_std.shape[1] # n of features\n",
    "n = Xtrain_std.shape[0] # n of observations (data points)\n",
    "\n",
    "I=range(n)\n",
    "J=range(p+1)\n",
    "L=range(1,p+1)\n",
    "\n",
    "# Parameters\n",
    "M=10000\n",
    "s=5 # features in the model\n",
    "\n",
    "# Create the decision variables\n",
    "beta = {}\n",
    "for j in J:\n",
    "    beta[j] = m.addVar(lb=-float('inf'), vtype=GRB.CONTINUOUS, name=f\"beta_{j}\")\n",
    "\n",
    "zeta = {}\n",
    "for j in L:\n",
    "    zeta[j] = m.addVar(vtype=GRB.BINARY, name=\"zeta\"+str(j))\n",
    "    \n",
    "# auxiliary to handle absolut values\n",
    "betap = m.addVars(L, lb=0.0, vtype=GRB.CONTINUOUS, name=\"betap\")\n",
    "betan = m.addVars(L, lb=0.0, vtype=GRB.CONTINUOUS, name=\"betan\")\n",
    "        \n",
    "errp = m.addVars(I, lb=0.0, vtype=GRB.CONTINUOUS, name=\"errn\")\n",
    "errn = m.addVars(I, lb=0.0, vtype=GRB.CONTINUOUS, name=\"errp\")\n",
    "\n",
    "\n",
    "# The objective is to minimize \n",
    "m.modelSense=gp.GRB.MINIMIZE\n",
    "m.setObjective(gp.quicksum(errp[i]+errn[i] for i in range(n)))\n",
    "\n",
    "\n",
    "for i in I:\n",
    "    m.addConstr(errp[i]-errn[i]==ytrain[i]-beta[0]-gp.quicksum(beta[j]*Xtrain_std[i][j-1] for j in L ))\n",
    "\n",
    "    \n",
    "m.addConstr(gp.quicksum(zeta[j] for j in L)>=p-s)\n",
    "\n",
    "for j in L:\n",
    "    m.addConstr(betap[j]-betan[j]==beta[j])\n",
    "    m.addConstr(betap[j]+betan[j]<=M*(1-zeta[j]))\n",
    "\n",
    "#for j in L:\n",
    "    # If zeta[i]=1, then beta[i] = 0\n",
    "    # m.addSOS(GRB.SOS_TYPE1, [zeta[j], beta[j]])\n",
    "    \n",
    "\n",
    "# Solve\n",
    "m.write(\"feat_sel.lp\")\n",
    "#m.display()\n",
    "m.optimize()\n",
    "\n",
    "\n",
    "if m.status == gp.GRB.status.OPTIMAL:\n",
    "    print('\\nSum of Absolute Residuals (Training Error): %g' % m.ObjVal)\n",
    "    print('\\nbetas:')\n",
    "    for j in range(p+1):\n",
    "        if math.fabs(beta[j].X)>0+0.0001:\n",
    "            print('%s %g' % (j, beta[j].X))\n",
    "    print('\\nzetas:')\n",
    "    for j in range(1,p+1):\n",
    "        #if zeta[j].X>0+0.0001:\n",
    "        print('%s %g' % (j, zeta[j].X))\n",
    "else:\n",
    "    print('No solution')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
